<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home Page on Community detection and analysis of American news agencies</title><link>https://rsm-git.github.io/</link><description>Recent content in Home Page on Community detection and analysis of American news agencies</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://rsm-git.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Data description</title><link>https://rsm-git.github.io/data-description/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rsm-git.github.io/data-description/</guid><description>The data used for this project is a subset of All the News 2.0, which contains 2,677,878 news articles American publications spanning from January 1, 2016 to April 2, 2020.
We chose to work with articles from the two publications with the most articles in the dataset: Reuters with 840,094 articles, and The New York Times (NYT) with 252,259 authors.
The dataset contains 10 features: authors, title, article text, url, section, publication and four features describing the time of publication.</description></item><item><title>Network analysis</title><link>https://rsm-git.github.io/network-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rsm-git.github.io/network-analysis/</guid><description>We construct the network based on the co-authorship of articles, to examine the collaborations of the news agencies. We construct a graph where each node represents an author and each edge represents a co-authorship between two authors. The network is undirected and unweighted. The size of the nodes are scaled based on the number of articles the author has written. We create a graph for both Reuters and The New York Times.</description></item><item><title>Text analysis</title><link>https://rsm-git.github.io/text-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rsm-git.github.io/text-analysis/</guid><description>We gathered all the articles from our two news agencies and grouped them by the agencies news sections. From here we removed stopwords and other unecesary abbreviations like Mr. and Ms. To normalize the data we ran the articles through a regex pattern then tokenized it for easy analysis.
As a starter we took a look at the top five terms by frequency from each section of the two news agencies.</description></item></channel></rss>