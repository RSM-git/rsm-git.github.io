---
title: Text analysis
prev: network-analysis
---

We gathered all the articles from our two news agencies and grouped them by the agencies news sections. From here we removed stopwords and other unecesary abbreviations like Mr. and Ms. To normalize the data we ran the articles through a regex pattern then tokenized it for easy analysis. 

As a starter we took a look at the top five terms by frequency from each section of the two news agencies. The table shows the top five for the section with most articles of the respective agency.

| Reuters - Business News - Top 5    |          NYT - US - Top 5         |
|:----------------------------------:|:---------------------------------:|
|                   US               |              Trump                |
|              Percent               |          President                |
|              Will                  |          New                      |
|              Year                  |          House                    |
|              Billion               |          One                      |

[('trump', 44909), ('president', 26847), ('new', 18349), ('house', 18237), ('one', 18074)]

[('us', 38276), ('percent', 35303), ('will', 33729), ('year', 32511), ('billion', 29966)]

We are interested in examining how the two publications portray the same subject. Here, we will use the sections to look for general tendencies in their reporting.
First, we need to determine which sections are concerned with the same subjects which we will do by looking at the word clouds generated by using tf-idf on the different sections.


|               Reuters               |               NYT               |
|:----------------------------------:|:---------------------------------:|
| ![](/images/Reuters_Section-Politics.png) | ![](/images/NYT_Section-us.png) |
| ![](/images/Reuters_Section-World-News.png) | ![](/images/NYT_Section-world.png) |

